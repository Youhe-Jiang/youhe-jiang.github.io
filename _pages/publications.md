---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

---
(* denotes equal contributions)  

**Revisiting the Time Cost Model of AllReduce**  
Dian Xiong, Li Chen, Youhe Jiang, Dan Li, Shuai Wang, Songtao Wang  
<span style="color:green; font-style:italic">Arxiv</span>   
| [paper](https://arxiv.org/pdf/2409.04202)
| [code]() |

**FlashFlex: Accommodating Large Language Model Training over Heterogeneous Environment**  
Ran Yan\*, **Youhe Jiang**\*, Wangcheng Tao, Xiaonan Nie, Bin Cui, Binhang Yuan   
<span style="color:green; font-style:italic">Arxiv</span>   
| [paper](https://arxiv.org/pdf/2409.01143)
| [code](https://github.com/Relaxed-System-Lab/FlashFlex) |

**HexGen: Generative Inference of Foundation Model over Heterogeneous Decentralized Environment**  
**Youhe Jiang**\*, Ran Yan\*, Xiaozhe Yao\*, Yang Zhou, Beidi Chen, Binhang Yuan   
<span style="color:green; font-style:italic">ICML 2024</span>   
| [paper](https://arxiv.org/pdf/2311.11514.pdf)
| [code](https://github.com/Relaxed-System-Lab/HexGen/tree/main) |

**Improving Automatic Parallel Training via Balanced Memory Workload Optimization**  
Yujie Wang, **Youhe Jiang**, Xupeng Miao, Fangcheng Fu, Shenhan Zhu, Xiaonan Nie, Yaofeng Tu, Bin Cui    
<span style="color:green; font-style:italic">TKDE 2024</span>   
| [paper](https://arxiv.org/pdf/2307.02031.pdf)
| [code](https://github.com/PKU-DAIR/Hetu/tree/main/tools/Galvatron) |

**OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning**  
**Youhe Jiang**, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Bin Cui  
<span style="color:green; font-style:italic">IJCAI 2023</span>  
| [paper](https://arxiv.org/pdf/2209.13258.pdf)
| [code](https://github.com/Youhe-Jiang/IJCAI2023-OptimalShardedDataParallel) |

**Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism**  
Xupeng Miao\*, Yujie Wang\*, **Youhe Jiang**\*, Chunan Shi, Xiaonan Nie, Hailin Zhang, Bin Cui    
<span style="color:green; font-style:italic">VLDB 2023</span>  
| [paper](https://arxiv.org/pdf/2211.13878.pdf)
| [code](https://github.com/PKU-DAIR/Hetu/tree/main/tools/Galvatron) |

**OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning**  
**Youhe Jiang**, Xupeng Miao, Xiaonan Nie, Bin Cui   
<span style="color:green; font-style:italic">ICML 2023 workshop</span>    
| [paper](https://www.dropbox.com/s/07lpaf4pdf7pza0/ICML_Workshop_Camera-ready.pdf?dl=0)
| [code](https://github.com/Youhe-Jiang/IJCAI2023-OptimalShardedDataParallel) |

**2D-HRA: Two-Dimensional Hierarchical Ring-Based All-Reduce Algorithm in Large-Scale Distributed Machine Learning**  
**Youhe Jiang**, Huaxi Gu, Yunfeng Lu, Xiaoshan Yu  
<span style="color:green; font-style:italic">IEEE Access 2020</span>  
| [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9211480)
| [code]() |
