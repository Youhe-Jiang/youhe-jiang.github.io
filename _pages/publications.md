---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

---
(* denotes equal contributions)  

**Hexgen-Text2SQL: Optimizing LLM Inference Request Scheduling for Agentic Text-to-SQL Workflow**
You Peng\*, **Youhe Jiang**\*, Chen Wang, Binhang Yuan 
<span style="color:green; font-style:italic">**Arxiv**</span>   
| [paper]()
| [code]() |

**Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs**  
**Youhe Jiang**\*, Fangcheng Fu\*, Xiaozhe Yao, Guoliang He, Xupeng Miao, Ana Klimovic, Bin Cui, Binhang Yuan, Eiko Yoneki  
<span style="color:green; font-style:italic">**ICML 2025**</span>   
| [paper]()
| [code]() |

**ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments**  
**Youhe Jiang**\*, Fangcheng Fu\*, Xiaozhe Yao\*, Taiyi Wang, Bin CUI, Ana Klimovic, Eiko Yoneki  
<span style="color:green; font-style:italic">**MLSys 2025**</span>   
| [paper]()
| [code]() |

**HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment**  
**Youhe Jiang**\*, Ran Yan\*, Binhang Yuan  
<span style="color:green; font-style:italic">**ICLR 2025**</span>   
| [paper]()
| [code]() |

**HexGen: Generative Inference of Foundation Model over Heterogeneous Decentralized Environment**  
**Youhe Jiang**\*, Ran Yan\*, Xiaozhe Yao\*, Yang Zhou, Beidi Chen, Binhang Yuan   
<span style="color:green; font-style:italic">**ICML 2024**</span>   
| [paper](https://arxiv.org/pdf/2311.11514.pdf)
| [code](https://github.com/Relaxed-System-Lab/HexGen/tree/main) |

**OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning**  
**Youhe Jiang**, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Bin Cui  
<span style="color:green; font-style:italic">**IJCAI 2023**</span>  
| [paper](https://arxiv.org/pdf/2209.13258.pdf)
| [code](https://github.com/Youhe-Jiang/IJCAI2023-OptimalShardedDataParallel) |

**Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism**  
Xupeng Miao\*, Yujie Wang\*, **Youhe Jiang**\*, Chunan Shi, Xiaonan Nie, Hailin Zhang, Bin Cui    
<span style="color:green; font-style:italic">**VLDB 2023**</span>  
| [paper](https://arxiv.org/pdf/2211.13878.pdf)
| [code](https://github.com/PKU-DAIR/Hetu/tree/main/tools/Galvatron) |

**Improving Automatic Parallel Training via Balanced Memory Workload Optimization**  
Yujie Wang, **Youhe Jiang**, Xupeng Miao, Fangcheng Fu, Shenhan Zhu, Xiaonan Nie, Yaofeng Tu, Bin Cui    
<span style="color:green; font-style:italic">**TKDE 2024**</span>   
| [paper](https://arxiv.org/pdf/2307.02031.pdf)
| [code](https://github.com/PKU-DAIR/Hetu/tree/main/tools/Galvatron) |

**HexiScale: Accommodating Large Language Model Training over Heterogeneous Environment**  
Ran Yan\*, **Youhe Jiang**\*, Xiaonan Nie, Fangcheng Fu, Bin Cui, Binhang Yuan   
<span style="color:green; font-style:italic">**Arxiv**</span>   
| [paper]()
| [code]() |

**Revisiting the Time Cost Model of AllReduce**  
Dian Xiong, Li Chen, **Youhe Jiang**, Dan Li, Shuai Wang, Songtao Wang  
<span style="color:green; font-style:italic">**Arxiv**</span>   
| [paper](https://arxiv.org/pdf/2409.04202)
| [code]() |

**OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning**  
**Youhe Jiang**, Xupeng Miao, Xiaonan Nie, Bin Cui   
<span style="color:green; font-style:italic">**ICML 2023 workshop**</span>    
| [paper](https://www.dropbox.com/s/07lpaf4pdf7pza0/ICML_Workshop_Camera-ready.pdf?dl=0)
| [code](https://github.com/Youhe-Jiang/IJCAI2023-OptimalShardedDataParallel) |

**2D-HRA: Two-Dimensional Hierarchical Ring-Based All-Reduce Algorithm in Large-Scale Distributed Machine Learning**  
**Youhe Jiang**, Huaxi Gu, Yunfeng Lu, Xiaoshan Yu  
<span style="color:green; font-style:italic">**IEEE Access 2020**</span>  
| [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9211480)
| [code]() |
